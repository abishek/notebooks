{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83013a24",
   "metadata": {},
   "source": [
    "## Classifiction Experiments Continued\n",
    "\n",
    "In this notebook, I plan to explore the same tweets dataset and the three classification algorithms but using word2vec and tfidf vector models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dabc0db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "# get the google news dataset model binary from here: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./models/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "dataset = 'datasets/tweets.csv'\n",
    "dataframe = pd.read_csv(dataset)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "82fb2c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "      <th>clean_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "      <td>[virginamerica, dhepburn, said]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>[virginamerica, plu, ad, commerci, experi, tacki]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "      <td>[virginamerica, todai, mean, need, trip]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>[virginamerica, aggress, blast, obnoxi, entert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>[virginamerica, big, bad, thing]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \\\n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)   \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)   \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)   \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)   \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)   \n",
       "\n",
       "                                         clean_words  \n",
       "0                    [virginamerica, dhepburn, said]  \n",
       "1  [virginamerica, plu, ad, commerci, experi, tacki]  \n",
       "2           [virginamerica, todai, mean, need, trip]  \n",
       "3  [virginamerica, aggress, blast, obnoxi, entert...  \n",
       "4                   [virginamerica, big, bad, thing]  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "\n",
    "    \n",
    "dataframe[\"clean_words\"] = dataframe[\"text\"].apply(preprocess_string)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7ae1cbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mean_vector(words: [str]):\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            vectors.append(model.get_vector(word, norm=True))\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return np.mean(vectors, axis=0) if len(vectors) > 0 else np.NaN\n",
    "\n",
    "dataframe[\"mv\"] = dataframe[\"clean_words\"].apply(generate_mean_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8c83dd6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14508, 17)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove rows that have a nan in mv column\n",
    "dataframe = dataframe[dataframe[\"mv\"].notna()]\n",
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c2d834bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14508,)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "target = LabelEncoder().fit_transform(dataframe[\"airline_sentiment\"])\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "85bb0d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14508, 300)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = dataframe[\"mv\"].apply(pd.Series)\n",
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b4d9c10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10881, 300) (10881,)\n",
      "(3627, 300) (3627,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_set, target, random_state=20, stratify=target)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6be74c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def predict(model, x_train, x_valid):\n",
    "    return {\n",
    "        \"train\": model.predict(x_train),\n",
    "        \"valid\": model.predict(x_valid),\n",
    "    }\n",
    "\n",
    "def accuracy(predictions, y_train, y_valid):\n",
    "    return {\n",
    "        \"train\": accuracy_score(y_train, predictions[\"train\"]),\n",
    "        \"valid\": accuracy_score(y_valid, predictions[\"valid\"])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4f272a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.7463468431210367, 'valid': 0.7419354838709677}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def logistic_regression(x_train, y_train, x_valid, y_valid):\n",
    "    lgmodel = LogisticRegression(max_iter=1000)\n",
    "    lgmodel.fit(x_train, y_train)\n",
    "\n",
    "    predictions = predict(lgmodel, x_train, x_valid)\n",
    "    return accuracy(predictions, y_train, y_valid)\n",
    "\n",
    "print(logistic_regression(x_train, y_train, x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dd4ade3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.9885120852862789, 'valid': 0.5985663082437276}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def decision_tree_classifier(x_train, y_train, x_valid, y_valid):\n",
    "    dtcmodel = DecisionTreeClassifier()\n",
    "    dtcmodel.fit(x_train, y_train)\n",
    "\n",
    "    predictions = predict(dtcmodel, x_train, x_valid)\n",
    "    return accuracy(predictions, y_train, y_valid)\n",
    "\n",
    "print(decision_tree_classifier(x_train, y_train, x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b294fde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.8312655086848635, 'valid': 0.7620623104494072}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "def svc(x_train, y_train, x_valid, y_valid):\n",
    "    svcmodel = svm.SVC()\n",
    "    svcmodel.fit(x_train, y_train)\n",
    "\n",
    "    predictions = predict(svcmodel, x_train, x_valid)\n",
    "    return accuracy(predictions, y_train, y_valid)\n",
    "\n",
    "print(svc(x_train, y_train, x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5348826",
   "metadata": {},
   "source": [
    "## Results so far\n",
    "\n",
    "Performance with gensim or plain spacy vectors are quite similar. Perhaps they just prove the equivalence of the English language and the fact that we haven't anything all that different. Next step is to repeat this with TfIdf Vectors and see if they differ from the above two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f6bd7573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-read the dataset. we dropped a few frames earlier. so let's restart from scratch for this.\n",
    "dataframe = pd.read_csv(dataset)\n",
    "dataframe.head()\n",
    "dataframe[\"clean_words\"] = dataframe[\"text\"].apply(preprocess_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3fc2693b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14508, 10150)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(dataframe[\"clean_words\"].apply(lambda x: ' '.join((map(str, x)))))\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "53983fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10881, 10150) (10881,)\n",
      "(3627, 10150) (3627,)\n"
     ]
    }
   ],
   "source": [
    "target = LabelEncoder().fit_transform(dataframe[\"airline_sentiment\"])\n",
    "target.shape\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(tfidf_matrix, target, random_state=20, stratify=target)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0a573718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.8652697362374782, 'valid': 0.7741935483870968}\n",
      "{'train': 0.9949453175259627, 'valid': 0.6845878136200717}\n",
      "{'train': 0.9518426615200809, 'valid': 0.775296388199614}\n"
     ]
    }
   ],
   "source": [
    "print(logistic_regression(x_train, y_train, x_valid, y_valid))\n",
    "print(decision_tree_classifier(x_train, y_train, x_valid, y_valid))\n",
    "print(svc(x_train, y_train, x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5e0eef",
   "metadata": {},
   "source": [
    "This is slightly better than the above results. But still not anywhere where I want. I think it should be possible to get to a 90% accuracy in a pretty straight forward fashion. Also, I think I'll drop DTC going forward. I need to read up and understand where the algorithm really works. Maybe am just using it all wrong here.\n",
    "\n",
    "Next step would be try and combine the above vectors in the same model and train them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "09450419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14508, 10450)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "combined_train_set = hstack([tfidf_matrix, csr_matrix(train_set)])\n",
    "combined_train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "938aaf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10881, 10150) (10881,)\n",
      "(3627, 10150) (3627,)\n"
     ]
    }
   ],
   "source": [
    "target = LabelEncoder().fit_transform(dataframe[\"airline_sentiment\"])\n",
    "target.shape\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(tfidf_matrix, target, random_state=20, stratify=target)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0d078c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.8652697362374782, 'valid': 0.7741935483870968}\n",
      "{'train': 0.9949453175259627, 'valid': 0.6873449131513648}\n",
      "{'train': 0.9518426615200809, 'valid': 0.775296388199614}\n"
     ]
    }
   ],
   "source": [
    "print(logistic_regression(x_train, y_train, x_valid, y_valid))\n",
    "print(decision_tree_classifier(x_train, y_train, x_valid, y_valid))\n",
    "print(svc(x_train, y_train, x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946d9f4e",
   "metadata": {},
   "source": [
    "One last thing to try before getting on meta-data is to use the glove embeddings. We are going to use their pre-trained model vectors. You can get a copy from [here](https://github.com/stanfordnlp/GloVe). Since this is twitter data, am going to use their twitter trained data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0afb27ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# am going to start from scratch for glove here.\n",
    "dataframe = pd.read_csv(dataset)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f23620",
   "metadata": {},
   "source": [
    "Let's try to load glove vectors instead of gensim vectors. glove has a vector set trained on twitter data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3fa5964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings = {}\n",
    "def load_glove(glove_file):\n",
    "    with open(glove_file, 'r') as gf:\n",
    "        for line in gf:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            glove_embeddings[word] = vector\n",
    "\n",
    "load_glove('models/glove/glove.twitter.27B.200d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ab32188c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1193514"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_embeddings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c96433ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "mention = re.compile('^@.*')\n",
    "hashtag = re.compile('^#.*')\n",
    "punctuation = re.compile('[!#,*$%^&.-]')\n",
    "digits = re.compile('^[0-9,\\.].*')\n",
    "urls = re.compile('^http.*')\n",
    "def cleanup(text:str) -> [str]:\n",
    "    # lowercase the text first\n",
    "    ltext = remove_stopwords(text.lower())\n",
    "    words = ltext.split(' ')\n",
    "    clean_words = []\n",
    "    # since these are tweets, remove any @ mentions. they can't really map meaningfully to lang vectors anyway.\n",
    "    for word in words:\n",
    "        # replace mentions with <user>\n",
    "        # replace hashtags with <hashtag>\n",
    "        # replace numbers with <number>\n",
    "        # replace urls with <url>\n",
    "        clean = re.sub(mention, '<user>', word)\n",
    "        clean = re.sub(hashtag, '<hashtag>', clean)\n",
    "        clean = re.sub(digits, '<number>', clean)\n",
    "        clean = re.sub(urls, '<url>', clean)\n",
    "        clean = re.sub(punctuation, '', clean)\n",
    "        if clean and clean not in stop_words:\n",
    "            clean_words.append(clean)\n",
    "    return clean_words if len(clean_words) > 0 else None\n",
    "\n",
    "\n",
    "dataframe[\"glove_clean\"] = dataframe[\"text\"].apply(cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "fdcaf308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_vector(list_of_tokens: [str]):\n",
    "    vectors = []\n",
    "    for token in list_of_tokens:\n",
    "        vec = glove_embeddings.get(token)\n",
    "        if vec is not None:\n",
    "            vectors.append(vec)\n",
    "\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    return None\n",
    "\n",
    "dataframe[\"glove_vectors\"] = dataframe[\"glove_clean\"].apply(glove_vector)\n",
    "\n",
    "#glove_vector(dataframe[\"glove_clean\"][3])\n",
    "#print(dataframe[\"glove_clean\"][3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "638c38b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 17)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove rows that have a nan in mv column\n",
    "dataframe = dataframe[dataframe[\"glove_vectors\"].notna()]\n",
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c1653472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14640,)\n",
      "(14640, 200)\n",
      "(10980, 200) (10980,)\n",
      "(3660, 200) (3660,)\n"
     ]
    }
   ],
   "source": [
    "target = LabelEncoder().fit_transform(dataframe[\"airline_sentiment\"])\n",
    "print(target.shape)\n",
    "\n",
    "train_set = dataframe[\"glove_vectors\"].apply(pd.Series)\n",
    "print(train_set.shape)\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_set, target, random_state=20, stratify=target)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1cc82c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.7828779599271403, 'valid': 0.7612021857923498}\n",
      "{'train': 0.9906193078324226, 'valid': 0.6232240437158469}\n",
      "{'train': 0.7960837887067396, 'valid': 0.773224043715847}\n"
     ]
    }
   ],
   "source": [
    "print(logistic_regression(x_train, y_train, x_valid, y_valid))\n",
    "print(decision_tree_classifier(x_train, y_train, x_valid, y_valid))\n",
    "print(svc(x_train, y_train, x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09bfdd8",
   "metadata": {},
   "source": [
    "Let's append this with tf-idf vectors and see if it gives us any edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "bb8e63fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 11238)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(dataframe[\"glove_clean\"].apply(lambda x: ' '.join((map(str, x)))))\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2abe1e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 11438)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_train_set = hstack([tfidf_matrix, csr_matrix(train_set)])\n",
    "combined_train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c71fee80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10980, 11238) (10980,)\n",
      "(3660, 11238) (3660,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(tfidf_matrix, target, random_state=20, stratify=target)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f1c77b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 0.8612932604735883, 'valid': 0.7734972677595628}\n",
      "{'train': 0.9939890710382514, 'valid': 0.6822404371584699}\n",
      "{'train': 0.9481785063752277, 'valid': 0.7601092896174864}\n"
     ]
    }
   ],
   "source": [
    "print(logistic_regression(x_train, y_train, x_valid, y_valid))\n",
    "print(decision_tree_classifier(x_train, y_train, x_valid, y_valid))\n",
    "print(svc(x_train, y_train, x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184f988a",
   "metadata": {},
   "source": [
    "So from all these exhaustive experiments, it seems using just a word vector or just the tfidf counts don't really get us over the 80% mark. In the next notebook, we'll continue using the glove model but we'l"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
